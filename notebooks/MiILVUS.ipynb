{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus==2.4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "try:\n",
    "    connections.connect(alias=\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: ['Hili_Trooper', 'Yair_Lapid', 'Hamad_Amar', 'Ariel_Kalner', 'Miri_Regev', 'Merav_Ben_Ari', 'Bezalel_Smotrich', 'Nir_Barkat', 'Moshe_Fassel', 'Zeev_Elkin', 'Simon_Davidson', 'Ron_Katz', 'Tsega_Maleko', 'Vladimir_Beliak', 'Sharon_Nir', 'Aryeh_Deri', 'Moshe_Tor_Paz', 'Yurai_Herzan', 'Hava_Atiya', 'Eli_Dallal', 'Amit_Halevi', 'Moshe_Saada', 'Boaz_Toporovsky', 'Aida_Sliman', 'Yuli_Edelstein', 'Karine_Elharrar', 'Benny_Gantz', 'Eliyahu_Beruchi', 'Moshe_Gafni', 'Yaakov_Tesler', 'Yonatan_Mishraki', 'Smicha_Rothman', 'Iman_Yassin', 'Yoav_Segalovitz', 'Moshe_Roth', 'Amir_Ohana', 'Debbie_Biton', 'Shalom_Danino', 'Youssef_Atauna', 'Nissim_Wattoori', 'Yosef_Tayeb', 'Almog_Cohen', 'Gilad_Kriv', 'Michal_Shir_Sagman', 'Yitzhak_Vserloff', 'Avigdor_Lieberman', 'Orit_Prakash_Cohen', 'Ram_Ben_Barak', 'Mai_Golan', 'Elazar_Stern', 'Yaakov_Asher', 'Moshe_Solomon', 'Osher_Shkalim', 'Uriel_Buso', 'Danny_Danon', 'Idan_Rol', 'Benjamin_Netanyahu', 'Yoav_Gallant', 'Mansour_Abbas', 'Ayman_Odeh', 'Yifat_Shasha_Biton', 'Gadi_Eisenkot', 'Dan_Illouz', 'Orna_Barbivai', 'Yulia_Malinovsky', 'Walid_Al_Huashla', 'Boaz_Bismuth', 'Avi_Dichter', 'Yitzhak_Pindros', 'Yitzhak_Kreiser', 'Tali_Gottlieb', 'Ofer_Katz', 'Naama_Lazimi', 'Sharon_Haskel', 'Israel_Katz', 'Moshe_Abutbul', 'Avichay_Buaron', 'Itamar_Ben_Gvir', 'Tatyana_Mazerski', 'Oded_Forer', 'Mati_Herchevi', 'Yinon_Azulai', 'Moshe_Arbel', 'Ohad_Tal', 'Chanoch_Melibitzki', 'Erez_Malul', 'David_Bitan', 'Merav_Cohen', 'Yariv_Levin', 'Orit_Struk', 'Tzvika_Fogel', 'Shlomo_Karai', 'Efrat_Marom', 'Walid_Taha', 'Yevgeny_Sova', 'Limor_Son_Har_Melech', 'Ahmed_Tibi', 'Yasmin_Friedman', 'Yisrael_Eichler', 'Michael_Biton', 'Michal_Veldigar', 'Ofer_Kasif', 'Pnina_Tamno', 'Micha_Malchieli', 'Kati_Shtrit', 'Naor_Shiri', 'Tzvi_Sukkot', 'Ofer_Super', 'Gila_Gamliel', 'Shelly_Tal_Miron', 'Avi_Maoz', 'Gideon_Saar', 'Matan_Kahane', 'Simon_Moshiashvili', 'Merav_Michaeli', 'Eliahu_Ravivo', 'Alon_Shuster', 'Sheshon_Guetta', 'Meir_Cohen', 'Miki_Levy']\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import utility\n",
    "collections = utility.list_collections()\n",
    "print(\"Available collections:\", collections)\n",
    "\n",
    "# for collection_name in collections:\n",
    "#   utility.drop_collection(collection_name)\n",
    "# print(\"All collections have been dropped.\")\n",
    "\n",
    "# collections = utility.list_collections()\n",
    "# print(\"Available collections:\", collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collection_exists(collection_name):\n",
    "    return utility.has_collection(collection_name)\n",
    "collection_exists(\"Miri_Regev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastic_ip = '34.0.64.248:9200'\n",
    "kibana_ip = '34.0.64.248:5601'\n",
    "es_username = 'user'\n",
    "es_password = 'knesset'\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# es = Elasticsearch(f'http://{elastic_ip}',http_auth=(es_username, es_password), timeout=1000000)\n",
    "# resp = es.search(index=\"all_features_sentences\", body={\"query\":{\"match_all\": {}}})\n",
    "# print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "# for hit in resp['hits']['hits']:\n",
    "#    print(\"id: %(sentence_id)s: speaker_name: %(speaker_name)s: sentence_text: %(sentence_text)s\" % hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(f'http://{elastic_ip}', basic_auth=(es_username, es_password), request_timeout=500)\n",
    "data_q =[]\n",
    "# Query definition\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"speaker_name\": \"איתמר בן גביר\"}}\n",
    "            ],\n",
    "            \"filter\": {\n",
    "                \"script\": {\n",
    "                    \"script\": {\n",
    "                        \"source\": \"doc['sentence_text.keyword'].size() > 0 && doc['sentence_text.keyword'].value.length() > 10\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize scroll\n",
    "resp = es.search(index=\"all_features_sentences\", body=query, scroll=\"2m\", size=1000)\n",
    "\n",
    "# Retrieve the scroll ID and first batch of hits\n",
    "scroll_id = resp['_scroll_id']\n",
    "hits = resp['hits']['hits']\n",
    "\n",
    "total_hits = 0\n",
    "while total_hits<4000:\n",
    "    for hit in hits:\n",
    "      data_q.append(\"%(sentence_text)s\" % hit[\"_source\"])\n",
    "      #print(\"%(sentence_text)s\" % hit[\"_source\"])\n",
    "\n",
    "    total_hits += len(hits)\n",
    "\n",
    "    # Fetch the next batch\n",
    "    resp = es.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "    scroll_id = resp['_scroll_id']\n",
    "    hits = resp['hits']['hits']\n",
    "\n",
    "print(f\"Total results retrieved: {total_hits}\")\n",
    "\n",
    "# Clear the scroll to free resources\n",
    "es.clear_scroll(scroll_id=scroll_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# for i in range(len(data_q)):\n",
    "#     data_q[i] = re.sub(r'[^א-ת ]', '', data_q[i]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_fn = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Encode the documents\n",
    "docs = data_q.copy()\n",
    "vectors = embedding_fn.encode(docs)  # Encode into vectors\n",
    "print(\"Dim:\", len(vectors[0]), vectors[0].shape)  # Dim: 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": i, \"vector\": vectors[i], \"text\": docs[i]}\n",
    "    for i in range(len(vectors))\n",
    "]\n",
    "print(\"Data has\", len(data), \"entities, each with fields:\", data[0].keys())\n",
    "print(\"Vector dim:\", len(data[0][\"vector\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "# Define the collection schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=len(vectors[0])),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=512),\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Demo collection for quotes of Miri Regev\")\n",
    "\n",
    "# Create or load the collection\n",
    "collection_name = \"Miri_Regev\"\n",
    "collection = Collection(name=collection_name, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milvus expects the data in columns (field-wise)\n",
    "field_data = [\n",
    "    [entity[\"id\"] for entity in data],  # IDs\n",
    "    [entity[\"vector\"] for entity in data],  # Vectors\n",
    "    [entity[\"text\"] for entity in data],  # Text\n",
    "]\n",
    "\n",
    "# Insert the data\n",
    "res = collection.insert(field_data)\n",
    "print(f\"Inserted {len(field_data[0])} entities into {collection_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index\n",
    "\n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "print(\"Index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import list_collections\n",
    "\n",
    "collections = list_collections()\n",
    "print(\"Collections in Milvus:\", collections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = Collection(name=\"Miri_Regev_quotes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()\n",
    "\n",
    "# Define a query vector (e.g., embedding for a new query)\n",
    "query_vector = embedding_fn.encode([\"מה דעתה תחבורה ציבורית?\"])[0]\n",
    "\n",
    "# Perform the search\n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "results = collection.search(\n",
    "    data=[query_vector],  # Query vector\n",
    "    anns_field=\"vector\",\n",
    "    param=search_params,\n",
    "    limit=15,  # Number of top matches\n",
    "    output_fields=[\"text\"],  # Include the text field in results\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    for hit in result:\n",
    "        print(f\"Text: {hit.entity.get('text')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_quotes(KNS_name, knesset_number):\n",
    "    es = Elasticsearch(f'http://{elastic_ip}', basic_auth=(es_username, es_password), request_timeout=500)\n",
    "    data_q = []\n",
    "\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"match\": {\"speaker_name\": KNS_name}},\n",
    "                    {\"match\": {\"knesset_number\": knesset_number}}\n",
    "                ],\n",
    "                \"filter\": {\n",
    "                    \"script\": {\n",
    "                        \"script\": {\n",
    "                            \"source\": \"doc['sentence_text.keyword'].size() > 0 && doc['sentence_text.keyword'].value.length() > 30\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialize scroll\n",
    "    resp = es.search(index=\"all_features_sentences\", body=query, scroll=\"2m\", size=4000)\n",
    "\n",
    "    # Retrieve the scroll ID and first batch of hits\n",
    "    scroll_id = resp['_scroll_id']\n",
    "    hits = resp['hits']['hits']\n",
    "\n",
    "    while hits:\n",
    "        for hit in hits:\n",
    "            data_q.append(\"%(sentence_text)s\" % hit[\"_source\"])\n",
    "        # Fetch the next batch\n",
    "        resp = es.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "        scroll_id = resp['_scroll_id']\n",
    "        hits = resp['hits']['hits']\n",
    "\n",
    "    print(f\"Total results retrieved: {len(data_q)}\")\n",
    "\n",
    "    # Clear the scroll to free resources\n",
    "    es.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "    for i in range(len(data_q)):\n",
    "        data_q[i] = re.sub(r'[^א-ת ]', '', data_q[i]).strip()\n",
    "\n",
    "    return data_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrive_quotes(\"מירי רגב\", \"23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "\n",
    "def retrive_quotes(KNS_name, knesset_number):\n",
    "\n",
    "    es = Elasticsearch(f'http://{elastic_ip}', basic_auth=(es_username, es_password), request_timeout=500)\n",
    "    data_q =[]\n",
    "    # Query definition\n",
    "    if knesset_number is not None:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"speaker_name\": KNS_name}},\n",
    "                        {\"match\": {\"knesset_number\": knesset_number}}\n",
    "                    ],\n",
    "                    \"filter\": {\n",
    "                        \"script\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"doc['sentence_text.keyword'].size() > 0 && doc['sentence_text.keyword'].value.length() > 30\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"speaker_name\": KNS_name}}\n",
    "                    ],\n",
    "                    \"filter\": {\n",
    "                        \"script\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"doc['sentence_text.keyword'].size() > 0 && doc['sentence_text.keyword'].value.length() > 30\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Initialize scroll\n",
    "    resp = es.search(index=\"all_features_sentences\", body=query, size=8000)\n",
    "    hits = resp['hits']['hits']\n",
    "\n",
    "    for hit in hits:\n",
    "        sentence = hit[\"_source\"].get(\"sentence_text\", \"\")\n",
    "        data_q.append(sentence)\n",
    "\n",
    "    print(f\"Total results retrieved: {len(data_q)}\")\n",
    "\n",
    "    # Clean sentences\n",
    "    for i in range(len(data_q)):\n",
    "        data_q[i] = re.sub(r'[^א-ת ]', '', data_q[i]).strip()\n",
    "\n",
    "    return data_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = retrive_quotes(\"משה אבוטבול\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Write the list to a CSV file\n",
    "with open('quotes.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Quote'])  # Add a header\n",
    "  for quote in d:\n",
    "    writer.writerow([quote])\n",
    "\n",
    "print(\"List has been written to quotes.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
